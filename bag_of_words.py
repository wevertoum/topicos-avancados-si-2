# -*- coding: utf-8 -*-
"""SI-2023-2 -Mineração de Textos - weverton - Exemplos de Pré-Processamento (Bag-of-Words).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_-YKchqWuuwh3nSeIVPs6PVC-mdmVLJD

# Instalando Bibliotecas
"""

# Importando bibliotecas
import pandas as pd
import string
import nltk
from google.colab import files
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('rslp')
from nltk.tokenize import word_tokenize
from nltk.stem.porter import *
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.spatial.distance import cosine
import numpy as np
import networkx as nx
!pip install plotly.express
!pip install pyexcel-ods
from plotly import graph_objs as go

"""# Exemplo de Remoção de Stopwords"""

nltk.corpus.stopwords.words('portuguese')

nltk.corpus.stopwords.words('english')

# remoção de pontuacao e stopwords

def remove_stopwords(text,lang,domain_stopwords=[]):

  stop_words = nltk.corpus.stopwords.words(lang) # lang='portuguese' or lang='english'

  s = str(text).lower() # tudo para caixa baixa
  table = str.maketrans({key: None for key in string.punctuation})
  s = s.translate(table) # remove pontuacao
  tokens = word_tokenize(s) #obtem tokens
  v = [i for i in tokens if not i in stop_words and not i in domain_stopwords and not i.isdigit()] # remove stopwords
  s = ""
  for token in v:
    s += token+" "
  return s.strip()


# exemplos de uso
text = "O estudante de Inteligência Artificial foi na livraria comprar  livros para estudar."
text2 = remove_stopwords(text, 'portuguese')
print('Antes: '+text)
print('Depois: '+text2)

"""# Exemplo de Stemming/Radicalização de Termos"""

# stemming
def stemming(text,lang):

  stemmer = PorterStemmer() # stemming para ingles

  if lang=='portuguese':
    stemmer = nltk.stem.RSLPStemmer() # stemming para portuguese

  tokens = word_tokenize(text) #obtem tokens

  sentence_stem = ''
  doc_text_stems = [stemmer.stem(i) for i in tokens]
  for stem in doc_text_stems:
    sentence_stem += stem+" "

  return sentence_stem.strip()


# exemplos de uso
text = "O estudante de Inteligência Artificial foi na livraria comprar livros para estudar."
text2 = remove_stopwords(text, 'portuguese')
text3 = stemming(text2, 'portuguese')
print('Antes: '+text)
print('Depois: '+text3)

"""# Coletando uma Base de Textos para Testar"""

import gdown
import pandas as pd

# URL compartilhada do Google Drive
drive_url = 'https://drive.google.com/uc?id=1aqxu_uMiVEkQ6RH4EP_7l6CNk6hdj_ir'

# Nome do arquivo no Google Drive
nome_do_arquivo = 'dados-brutos-aspectj.csv'

# Baixar o arquivo do Google Drive
gdown.download(drive_url, nome_do_arquivo, quiet=False)

# Ler o arquivo como um DataFrame
df = pd.read_csv(nome_do_arquivo, sep=',', on_bad_lines='skip')

# Filtrar as linhas da coluna 'description_stemmed' que não sejam nulas ou vazias
df = df.dropna(subset=['description_stemmed'])

# Exibir apenas a coluna 'description_stemmed'
descriptions = df['description_stemmed']

# Exibir as descrições de bugs
for description in descriptions:
    print(description)

df.reset_index(drop=True,inplace=True)

df

"""[texto do link](https://)# Computando uma Bag-of-Words"""

from sklearn.feature_extraction.text import CountVectorizer

# Limite o número máximo de documentos (linhas) que serão considerados
max_documents = 6000  # Defina o número desejado

# Crie um objeto CountVectorizer com o limite
vectorizer = CountVectorizer(max_features=max_documents)

X = vectorizer.fit_transform(descriptions)

feature_names = vectorizer.get_feature_names_out()

# Crie um DataFrame para a Bag of Words
bag_of_words_df = pd.DataFrame(X.toarray(), columns=feature_names)

# Exiba a Bag of Words
print(bag_of_words_df)

list(df.columns)

"""# Dissimilaridade de cosseno (1-cos) entre dois textos"""

# computando dissimilaridade de cosseno

def dis_cosine(matrix, e1, e2):
  dcos = cosine(matrix.iloc[e1,:], matrix.iloc[e2,:])
  return dcos


# exemplo: dissimilaride entre o primeiro (id=0) e o segundo evento (id=1) do vsm-tfidf
dis_cosine(vsm,1,2)

df.iloc[1]

df.iloc[2]

vsm

# dissimilaridade entre documento 1 e os outros 1 até 10

for i in range(1,11):
  print('doc=',i,dis_cosine(vsm,1,i))

input = 4
print('Entrada =',df.iloc[input])
for index2,row2 in vsm.iterrows():
    dcos = dis_cosine(vsm,input,index2)
    if dcos > 0 and dcos <= 0.6:
      text = df.iloc[index2]
      print(input,index2,dcos,text)

df.iloc[0]

"""# Resposta do Exercício - Aula 01"""

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Função de tokenização
def meu_tokenizador(doc):
    # Separa as palavras por espaços em branco
    tokens = doc.split()
    return tokens

# Vetorizador TF-IDF
tfidf_vectorizer = TfidfVectorizer(tokenizer=meu_tokenizador)

# Ajustar e transformar o texto
tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])

# Criar um DataFrame para representação TF-IDF
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Exibir a representação TF-IDF
print(tfidf_df)

"""## Parte 2"""

vsm

list(pd.DataFrame(vsm.iloc[1]).sort_values(by=1,ascending=False).head(5).index)

from sklearn.neighbors import kneighbors_graph
A = kneighbors_graph(vsm, 2, metric='cosine')
A.toarray()

import networkx as nx

G = nx.Graph(A.toarray())

from tqdm.notebook import tqdm
L_edges = []
for edge in tqdm(G.edges()):
  L_edges.append([edge[0],edge[1],1])

df_edges = pd.DataFrame(L_edges)
df_edges.columns = ['source', 'target', 'value']
df_edges

import re
L_nodes = []
for node in G.nodes():
  L_nodes.append([node,list(pd.DataFrame(vsm.iloc[node]).sort_values(by=node,ascending=False).head(5).index)])

df_nodes = pd.DataFrame(L_nodes)
df_nodes.columns = ['id','text']
df_nodes

df_edges.to_csv('edges.csv',sep=',')
df_nodes.to_csv('nodes.csv',sep=',')



"""### Exportar

Os arquivos edges.csv e nodes.csv podem ser visualizados e explorados em ferramentas de análise de grafos.
Por exemplo:
https://cosmograph.app/run/

# Agrupamento de Textos usando k-Means
"""

from sklearn.cluster import KMeans
import numpy as np

num_cluster = 3

X = np.array(vsm)
length = np.sqrt((X**2).sum(axis=1))[:,None]
X = X / length

kmeans = KMeans(n_clusters=num_cluster, random_state=0, n_init=10).fit(X)

### Quais foram os representantes (centroides) de cada grupo?
pd.DataFrame(kmeans.cluster_centers_)

### Quais foram os clusters?

pd.DataFrame(kmeans.labels_)

import re
L_nodes = []
for node in G.nodes():
  L_nodes.append([node,list(pd.DataFrame(vsm.iloc[node]).sort_values(by=node,ascending=False).head(5).index),'cluster_'+str(kmeans.labels_[node])])

df_nodes = pd.DataFrame(L_nodes)
df_nodes.columns = ['id','text','cluster']
df_nodes

df_nodes.to_csv('nodes.csv',sep=',')

